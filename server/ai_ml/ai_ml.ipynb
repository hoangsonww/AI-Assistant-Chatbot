{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde9b46c",
   "metadata": {},
   "source": [
    "# RAG Pipeline with OpenAI and FAISS\n",
    "\n",
    "This section demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline that:\n",
    "  - Loads text documents from a folder (e.g. `data`).\n",
    "  - Creates document embeddings using OpenAI.\n",
    "  - Builds a FAISS vector store for similarity search.\n",
    "  - Sets up a RetrievalQA chain to answer a sample query.\n",
    "\n",
    "**File Name:** `rag_pipeline.py`"
   ]
  },
  {
   "cell_type": "code",
   "id": "b744e68b",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Ensure the OpenAI API key is set\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    raise ValueError('Please set the OPENAI_API_KEY environment variable.')\n",
    "\n",
    "def load_documents(directory):\n",
    "    \"\"\"\n",
    "    Loads all text documents (.txt files) from the given directory.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n",
    "        loader = TextLoader(file_path)\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "# 1. Load documents from the 'data' folder\n",
    "docs = load_documents('data')\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# 2. Initialize OpenAI embeddings (this calls the OpenAI API)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# 3. Build a FAISS vector store from the documents\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "print(\"Vector store created with FAISS.\")\n",
    "\n",
    "# 4. Create a RetrievalQA chain using OpenAI LLM\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(openai_api_key=openai_api_key, temperature=0),\n",
    "    chain_type=\"stuff\",  # Concatenates retrieved docs\n",
    "    retriever=vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    ")\n",
    "\n",
    "# 5. Run a sample query\n",
    "query = \"What is the main idea discussed in these documents?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4f46f",
   "metadata": {},
   "source": [
    "# Document QA with Local Embeddings\n",
    "\n",
    "This section demonstrates a document question-answering system that uses local embeddings from a Sentence Transformer model. It:\n",
    "  - Loads documents recursively from a folder (e.g. `docs`).\n",
    "  - Creates embeddings using the `all-MiniLM-L6-v2` model.\n",
    "  - Builds a FAISS vector store for similarity search.\n",
    "  - Uses OpenAI’s LLM to answer a question based on the retrieved documents.\n",
    "\n",
    "**File Name:** `local_document_qa.py`"
   ]
  },
  {
   "cell_type": "code",
   "id": "b26c4a7a",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load documents recursively from the 'docs' directory (all .txt files)\n",
    "loader = DirectoryLoader('docs', glob='**/*.txt')\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents from the 'docs' directory.\")\n",
    "\n",
    "# Initialize local embeddings using Sentence Transformers\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Build a FAISS vector store from the documents\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "print(\"FAISS vector store created using local embeddings.\")\n",
    "\n",
    "# Create a RetrievalQA chain with OpenAI LLM\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),  # Uses the OPENAI_API_KEY from your environment\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    ")\n",
    "\n",
    "# Run a sample question\n",
    "question = \"Can you summarize the key points in the documents?\"\n",
    "answer = qa_chain.run(question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e246e",
   "metadata": {},
   "source": [
    "# Conversational Chatbot with Retrieval-Augmented Generation\n",
    "\n",
    "This section builds an interactive chatbot that uses a conversational retrieval chain. The chatbot:\n",
    "  - Loads documents from a folder (e.g. `chat_data`).\n",
    "  - Creates embeddings using OpenAI’s model.\n",
    "  - Builds a FAISS vector store for retrieval.\n",
    "  - Uses a conversation memory to maintain context across interactions.\n",
    "\n",
    "**File Name:** `conversational_chatbot.py`"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e82a850",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Ensure the OpenAI API key is set\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    raise ValueError('Please set the OPENAI_API_KEY environment variable.')\n",
    "\n",
    "def load_documents(directory):\n",
    "    \"\"\"\n",
    "    Loads all text documents (.txt files) from the specified directory.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n",
    "        loader = TextLoader(file_path)\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "# Load documents from the 'chat_data' folder\n",
    "docs = load_documents('chat_data')\n",
    "print(f\"Loaded {len(docs)} documents for chatbot retrieval.\")\n",
    "\n",
    "# Create embeddings using OpenAI\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Build a FAISS vector store\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "print(\"FAISS vector store created for retrieval.\")\n",
    "\n",
    "# Initialize conversation memory to store chat history\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Create a conversational retrieval chain that integrates:\n",
    "#  - OpenAI LLM (with some creativity via temperature)\n",
    "#  - The FAISS vector store retriever\n",
    "#  - Conversational memory\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=OpenAI(openai_api_key=openai_api_key, temperature=0.7),\n",
    "    retriever=vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}),\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Simulate a conversation (in a real-world scenario, you might use input())\n",
    "print(\"Chatbot is ready! Simulated conversation:\")\n",
    "\n",
    "# First query\n",
    "user_input = \"Tell me about the documents.\"\n",
    "response = conv_chain.run(user_input)\n",
    "print(\"User:\", user_input)\n",
    "print(\"Bot:\", response)\n",
    "\n",
    "# Second query with conversational context\n",
    "user_input = \"Can you provide more details?\"\n",
    "response = conv_chain.run(user_input)\n",
    "print(\"User:\", user_input)\n",
    "print(\"Bot:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
